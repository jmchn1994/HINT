{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import analysis\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import scipy.stats as ss\n",
    "\n",
    "# id, sessionConfig, sessionResp, finalQuestions, summary, survey\n",
    "batch_1 = [w for w in analysis.readWorkers('commitment-fixed/Batch_0.csv', 'commitment')]\n",
    "batch_2 = [w for w in analysis.readWorkers('commitment-fixed/Batch_1.csv', 'commitment')]\n",
    "batch_3 = [w for w in analysis.readWorkers('commitment-fixed/Batch_2.csv', 'commitment')]\n",
    "batch_4 = [w for w in analysis.readWorkers('commitment-fixed/Batch_3.csv', 'commitment')]\n",
    "batch_5 = [w for w in analysis.readWorkers('commitment-fixed/Batch_4.csv', 'commitment')]\n",
    "raw_workers = batch_1 + batch_2 + batch_3 + batch_4 + batch_5\n",
    "workers = analysis.filter(raw_workers, 'commitment')\n",
    "print(f'There are {len(workers)} workers who passed quality check out of {len(raw_workers)}')\n",
    "ids = [id for id, _, _, _, _, _ in workers]\n",
    "# Assert set uniqueness\n",
    "print(f'{len(set(ids))} unique workers')\n",
    "idset = set()\n",
    "for i, id in enumerate(ids):\n",
    "    if not id in idset:\n",
    "        idset.add(id)\n",
    "    else:\n",
    "        print(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = [s for id, _, _, fq, s, _ in workers if s['']['perf-pattern'] == 'full,full,full,full']\n",
    "print(f'{len(group)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivotAndAggregate(df, values, index):\n",
    "    renameMap = {}\n",
    "    if isinstance(values, list):\n",
    "        for name in values:\n",
    "            renameMap[name] = name + '_err'\n",
    "    else:\n",
    "        renameMap[values] = values + '_err'\n",
    "    df_p = df.pivot_table(values=values, aggfunc='mean', index=index)\n",
    "    df_pe = df.pivot_table(values=values, aggfunc=ss.sem, index=index).rename(columns=renameMap)\n",
    "    return df_p.join(df_pe).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freeform Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "explanation = [fq['explanation'] for id, _, _, fq, _, _ in workers]\n",
    "print('\\n\\n'.join(explanation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance over sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = [(id, e, s['']['perf-pattern'], analysis.f1(s[e]), s[e]['read'] / s[e]['total'])  for e in analysis.EXPERIMENTS for id, cfg, rsp, q, s, sr in workers]\n",
    "df_f1 = pd.DataFrame(f1_scores, columns=('id', 'exp', 'cond', 'f1', 'read'))\n",
    "df_f1_agg = pivotAndAggregate(df_f1, values=['f1', 'read'], index=['exp', 'cond'])\n",
    "fig = px.line(df_f1, x='exp', y='f1', color='id', range_y = [0, 1.1], facet_col = 'cond')\n",
    "fig.show()\n",
    "fig = px.line(df_f1_agg, x='exp', y='f1', range_y = [0, 1.1], color = 'cond', error_y = 'f1_err')\n",
    "for d in fig.data:\n",
    "    d.update(mode='markers+lines')\n",
    "fig.show()\n",
    "df_f1_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_f1, x='exp', y='read', color='id', range_y = [0, 1.1], facet_col = 'cond')\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(df_f1_agg, x='exp', y='read', range_y = [0, 1.1], color = 'cond', error_y = 'read_err')\n",
    "for d in fig.data:\n",
    "    d.update(mode='markers+lines')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_questions = ['preference', 'understand']\n",
    "overall = [(id, q, s['']['perf-pattern'], int(fq[q])) \n",
    "                for q in overall_questions\n",
    "                for id, _, _, fq, s, _ in workers]\n",
    "df_overall = pd.DataFrame(overall, columns=('id', 'question', 'cond', 'answer'))\n",
    "fig = px.box(df_overall, x='cond', y='answer', points=\"all\", facet_col='question')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_understand = df_overall[df_overall['question'] == 'understand']\n",
    "answers = [0, 0, 0, 0, 0, 0, 0]\n",
    "for row in q_understand.itertuples():\n",
    "    answers[row.answer - 1] += 1\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survey Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "questions = [\"confidence\", \"effort\", \"utility\", \"mentalmodel\", \"trust\", \"stickiness\"]\n",
    "\n",
    "surveys = [(id, e, q, s['']['perf-pattern'], int(sr[e][q]) if q in sr[e] else None) \n",
    "               for q in questions \n",
    "               for e in analysis.EXPERIMENTS[:]\n",
    "               for id, _, _, _, s, sr in workers]\n",
    "df_survey = pd.DataFrame(surveys, columns=('id', 'exp', 'question', 'cond', 'answer'))\n",
    "df_survey_agg = pivotAndAggregate(df_survey, 'answer', ['exp', 'question', 'cond'])\n",
    "fig = px.line(df_survey_agg, x='exp', y='answer', facet_col='question', color='cond', range_y=[1, 7], error_y = 'answer_err')\n",
    "for d in fig.data:\n",
    "    d.update(mode='markers+lines')\n",
    "#fig = px.violin(df_survey, x='exp', y='answer', facet_col='question', color='cond')\n",
    "#fig = px.box(df_survey, x='exp', y='answer', facet_col='question', color='cond', points='all')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time and effort on task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = [(id, e, s['']['perf-pattern'], s[e]['log']['taskTime'], s[e]['log']['view']) for e in analysis.EXPERIMENTS for id, cfg, rsp, q, s, sr in workers]\n",
    "df_time = pd.DataFrame(time, columns=('id', 'exp', 'cond', 'time', 'view'))\n",
    "df_time_agg =pivotAndAggregate(df_time, values=['time', 'view'], index=['exp', 'cond'])\n",
    "fig = px.line(df_time, x='exp', y='time', color='id', facet_col='cond')\n",
    "fig.show()\n",
    "fig = px.line(df_time_agg, x='exp', y='time', color='cond', error_y = 'time_err')\n",
    "for d in fig.data:\n",
    "    d.update(mode='markers+lines')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_time, x='exp', y='view', color='id', facet_col='cond')\n",
    "fig.show()\n",
    "fig = px.line(df_time_agg, x='exp', y='view', color='cond', error_y = 'view_err')\n",
    "for d in fig.data:\n",
    "    d.update(mode='markers+lines')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = [(id, e, s['']['perf-pattern'], s[e]['uptake'], s[e]['corrections']) for e in analysis.EXPERIMENTS for id, cfg, rsp, q, s, sr in workers if not e[:4] == 'cali']\n",
    "df_uptake = pd.DataFrame(time, columns=('id', 'exp', 'cond', 'uptake', 'corrections'))\n",
    "fig = px.box(df_uptake, x='exp', y='uptake', facet_col='cond')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uptake_agg = pivotAndAggregate(df_uptake, values=['uptake', 'corrections'], index=['exp', 'cond'])\n",
    "fig = px.line(df_uptake_agg, x='exp', y='uptake', color='cond', error_y = 'uptake_err')\n",
    "for d in fig.data:\n",
    "    d.update(mode='markers+lines')\n",
    "fig.show()\n",
    "df_uptake_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(df_uptake, x='exp', y='corrections', color='id', facet_col='cond')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_uptake_agg, x='exp', y='corrections', color='cond', error_y = 'corrections_err')\n",
    "for d in fig.data:\n",
    "    d.update(mode='markers+lines')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
